{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary-Classification with ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aim of This Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify reviews as positive of negative easily helps company to see negative reviewed products easily without reading too much reviews. So, my focus point is negative rated products. So, I divided my target as 0 and 1. 3 and above rated books were collected together as 1 and 1-2 were classified as 0.\n",
    "\n",
    "My aim in this notebook is to predict reviews as positive or negative from text. To do this, I used machine learning algoritms. Also, deep learning solutions can be found in same repo. \n",
    "\n",
    "### Metric:\n",
    "\n",
    "As metric, I will use balanced accuracy values, but I will also look to confusion matrix to decide because my concern is to predict 0 (negative) reviews more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlxtend\n",
      "  Downloading mlxtend-0.19.0-py2.py3-none-any.whl (1.3 MB)\n",
      "Requirement already satisfied: scikit-learn>=0.20.3 in c:\\users\\swammy\\anaconda3\\lib\\site-packages (from mlxtend) (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\swammy\\anaconda3\\lib\\site-packages (from mlxtend) (1.19.5)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\users\\swammy\\anaconda3\\lib\\site-packages (from mlxtend) (1.0.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\swammy\\anaconda3\\lib\\site-packages (from mlxtend) (49.2.0.post20200714)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\users\\swammy\\anaconda3\\lib\\site-packages (from mlxtend) (0.16.0)\n",
      "Requirement already satisfied: scipy>=1.2.1 in c:\\users\\swammy\\anaconda3\\lib\\site-packages (from mlxtend) (1.5.0)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\users\\swammy\\anaconda3\\lib\\site-packages (from mlxtend) (3.2.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\swammy\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\swammy\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\swammy\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\swammy\\anaconda3\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.8.1)\n",
      "Requirement already satisfied: six in c:\\users\\swammy\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib>=3.0.0->mlxtend) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\swammy\\anaconda3\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2020.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\swammy\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.3->mlxtend) (2.1.0)\n",
      "Installing collected packages: mlxtend\n",
      "Successfully installed mlxtend-0.19.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\swammy\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\swammy\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\swammy\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\swammy\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\swammy\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\swammy\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\swammy\\anaconda3\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.2.2; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the 'c:\\users\\swammy\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.4.2-py3-none-win_amd64.whl (97.8 MB)\n",
      "Requirement already satisfied: scipy in c:\\users\\swammy\\anaconda3\\lib\\site-packages (from xgboost) (1.5.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\swammy\\anaconda3\\lib\\site-packages (from xgboost) (1.19.5)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.4.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\swammy\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\swammy\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\swammy\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\swammy\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\swammy\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\swammy\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\swammy\\anaconda3\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.2.2; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the 'c:\\users\\swammy\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\swammy\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\swammy\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\swammy\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\swammy\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\swammy\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\swammy\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\swammy\\anaconda3\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.2.2; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the 'c:\\users\\swammy\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading lightgbm-3.2.1-py3-none-win_amd64.whl (1.0 MB)\n",
      "Requirement already satisfied: scipy in c:\\users\\swammy\\anaconda3\\lib\\site-packages (from lightgbm) (1.5.0)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\users\\swammy\\anaconda3\\lib\\site-packages (from lightgbm) (0.24.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\swammy\\anaconda3\\lib\\site-packages (from lightgbm) (1.19.5)\n",
      "Requirement already satisfied: wheel in c:\\users\\swammy\\anaconda3\\lib\\site-packages (from lightgbm) (0.36.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\swammy\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (0.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\swammy\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (2.1.0)\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-3.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe and series \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# sklearn imports for modeling part\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score,balanced_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from mlxtend.evaluate import confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# To plot\n",
    "import matplotlib.pyplot as plt  \n",
    "%matplotlib inline    \n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# XGBoost and LGBM classifier imports\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns=100 # To see the hidden columns in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample30.csv', low_memory=False) #taking cleaned data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>brand</th>\n",
       "      <th>categories</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>name</th>\n",
       "      <th>reviews_date</th>\n",
       "      <th>reviews_didPurchase</th>\n",
       "      <th>reviews_doRecommend</th>\n",
       "      <th>reviews_rating</th>\n",
       "      <th>reviews_text</th>\n",
       "      <th>reviews_title</th>\n",
       "      <th>reviews_userCity</th>\n",
       "      <th>reviews_userProvince</th>\n",
       "      <th>user_sentiment</th>\n",
       "      <th>review_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AV13O1A8GV-KLJ3akUyj</td>\n",
       "      <td>Universal Music</td>\n",
       "      <td>Movies, Music &amp; Books,Music,R&amp;b,Movies &amp; TV,Mo...</td>\n",
       "      <td>Universal Music Group / Cash Money</td>\n",
       "      <td>Pink Friday: Roman Reloaded Re-Up (w/dvd)</td>\n",
       "      <td>2012-11-30T06:21:45.000Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>i love this album. it's very good. more to the...</td>\n",
       "      <td>Just Awesome</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>i love this album its very good more to the hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AV14LG0R-jtxr-f38QfS</td>\n",
       "      <td>Lundberg</td>\n",
       "      <td>Food,Packaged Foods,Snacks,Crackers,Snacks, Co...</td>\n",
       "      <td>Lundberg</td>\n",
       "      <td>Lundberg Organic Cinnamon Toast Rice Cakes</td>\n",
       "      <td>2017-07-09T00:00:00.000Z</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>good flavor. this review was collected as part...</td>\n",
       "      <td>Good</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>good flavor this review was collected as part ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AV14LG0R-jtxr-f38QfS</td>\n",
       "      <td>Lundberg</td>\n",
       "      <td>Food,Packaged Foods,Snacks,Crackers,Snacks, Co...</td>\n",
       "      <td>Lundberg</td>\n",
       "      <td>Lundberg Organic Cinnamon Toast Rice Cakes</td>\n",
       "      <td>2017-07-09T00:00:00.000Z</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>good flavor.</td>\n",
       "      <td>Good</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>good flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AV16khLE-jtxr-f38VFn</td>\n",
       "      <td>K-Y</td>\n",
       "      <td>Personal Care,Medicine Cabinet,Lubricant/Sperm...</td>\n",
       "      <td>K-Y</td>\n",
       "      <td>K-Y Love Sensuality Pleasure Gel</td>\n",
       "      <td>2016-01-06T00:00:00.000Z</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>i read through the reviews on here before look...</td>\n",
       "      <td>Disappointed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>i read through the reviews on here before look...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AV16khLE-jtxr-f38VFn</td>\n",
       "      <td>K-Y</td>\n",
       "      <td>Personal Care,Medicine Cabinet,Lubricant/Sperm...</td>\n",
       "      <td>K-Y</td>\n",
       "      <td>K-Y Love Sensuality Pleasure Gel</td>\n",
       "      <td>2016-12-21T00:00:00.000Z</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>my husband bought this gel for us. the gel cau...</td>\n",
       "      <td>Irritation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>my husband bought this gel for us the gel caus...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id            brand  \\\n",
       "0  AV13O1A8GV-KLJ3akUyj  Universal Music   \n",
       "1  AV14LG0R-jtxr-f38QfS         Lundberg   \n",
       "2  AV14LG0R-jtxr-f38QfS         Lundberg   \n",
       "3  AV16khLE-jtxr-f38VFn              K-Y   \n",
       "4  AV16khLE-jtxr-f38VFn              K-Y   \n",
       "\n",
       "                                          categories  \\\n",
       "0  Movies, Music & Books,Music,R&b,Movies & TV,Mo...   \n",
       "1  Food,Packaged Foods,Snacks,Crackers,Snacks, Co...   \n",
       "2  Food,Packaged Foods,Snacks,Crackers,Snacks, Co...   \n",
       "3  Personal Care,Medicine Cabinet,Lubricant/Sperm...   \n",
       "4  Personal Care,Medicine Cabinet,Lubricant/Sperm...   \n",
       "\n",
       "                         manufacturer  \\\n",
       "0  Universal Music Group / Cash Money   \n",
       "1                            Lundberg   \n",
       "2                            Lundberg   \n",
       "3                                 K-Y   \n",
       "4                                 K-Y   \n",
       "\n",
       "                                         name              reviews_date  \\\n",
       "0   Pink Friday: Roman Reloaded Re-Up (w/dvd)  2012-11-30T06:21:45.000Z   \n",
       "1  Lundberg Organic Cinnamon Toast Rice Cakes  2017-07-09T00:00:00.000Z   \n",
       "2  Lundberg Organic Cinnamon Toast Rice Cakes  2017-07-09T00:00:00.000Z   \n",
       "3            K-Y Love Sensuality Pleasure Gel  2016-01-06T00:00:00.000Z   \n",
       "4            K-Y Love Sensuality Pleasure Gel  2016-12-21T00:00:00.000Z   \n",
       "\n",
       "  reviews_didPurchase reviews_doRecommend  reviews_rating  \\\n",
       "0                 NaN                 NaN               5   \n",
       "1                True                 NaN               5   \n",
       "2                True                 NaN               5   \n",
       "3               False               False               1   \n",
       "4               False               False               1   \n",
       "\n",
       "                                        reviews_text reviews_title  \\\n",
       "0  i love this album. it's very good. more to the...  Just Awesome   \n",
       "1  good flavor. this review was collected as part...          Good   \n",
       "2                                       good flavor.          Good   \n",
       "3  i read through the reviews on here before look...  Disappointed   \n",
       "4  my husband bought this gel for us. the gel cau...    Irritation   \n",
       "\n",
       "  reviews_userCity reviews_userProvince  user_sentiment  \\\n",
       "0      Los Angeles                  NaN               2   \n",
       "1              NaN                  NaN               2   \n",
       "2              NaN                  NaN               2   \n",
       "3              NaN                  NaN               0   \n",
       "4              NaN                  NaN               0   \n",
       "\n",
       "                                        review_clean  \n",
       "0  i love this album its very good more to the hi...  \n",
       "1  good flavor this review was collected as part ...  \n",
       "2                                        good flavor  \n",
       "3  i read through the reviews on here before look...  \n",
       "4  my husband bought this gel for us the gel caus...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have already cleaned my data but after converting my data to new column, I would like to make sure my new column is clean or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                          0\n",
       "brand                       0\n",
       "categories                  0\n",
       "manufacturer              141\n",
       "name                        0\n",
       "reviews_date               46\n",
       "reviews_didPurchase     13877\n",
       "reviews_doRecommend      2404\n",
       "reviews_rating              0\n",
       "reviews_text                0\n",
       "reviews_title               0\n",
       "reviews_userCity        27881\n",
       "reviews_userProvince    29642\n",
       "user_sentiment              0\n",
       "review_clean                1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()  # to check cleaned column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['review_clean'], inplace=True) #droping null's in review clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                          0\n",
       "brand                       0\n",
       "categories                  0\n",
       "manufacturer              141\n",
       "name                        0\n",
       "reviews_date               46\n",
       "reviews_didPurchase     13877\n",
       "reviews_doRecommend      2404\n",
       "reviews_rating              0\n",
       "reviews_text                0\n",
       "reviews_title               0\n",
       "reviews_userCity        27880\n",
       "reviews_userProvince    29641\n",
       "user_sentiment              0\n",
       "review_clean                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum() #to check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking Samples for Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My target is highly unbalanced. To teach my model more about minority class, I will take sample data from each classes balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_two_sentiment(reviews_rating):\n",
    "    '''This function encodes the rating 1 and 2 as 0, others as 1'''\n",
    "    if reviews_rating >= 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['user_sentiment'] = df['reviews_rating'].apply(calc_two_sentiment) # appyling converter to change sentiments from 3 to 2-classed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    28025\n",
       "0     1783\n",
       "Name: user_sentiment, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['user_sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = df.loc[df['user_sentiment']==0].head(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df.loc[df['user_sentiment']==1].head(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df_0, df_1]\n",
    "#getting together samples\n",
    "df_sampled = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    28025\n",
       "0     1783\n",
       "Name: user_sentiment, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sampled['user_sentiment'].value_counts() #sample classes - balanced chosen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use functions for my modeling and spliting parts because I want to get results easily when I changed something in model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_split(df) :\n",
    "    '''This function splits data to train and test, then vectorized reviews '''\n",
    "    \n",
    "    # split train-test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df['review_clean'], \n",
    "                                                        df['user_sentiment'], test_size=0.2, random_state=42)\n",
    "    print(X_train.shape)\n",
    "    print(X_test.shape)\n",
    "        \n",
    "    # define vectorize and fit to data     \n",
    "    word_vectorizer = TfidfVectorizer(sublinear_tf=True,strip_accents='unicode',\n",
    "        analyzer='word',token_pattern=r'\\w{1,}',stop_words='english',\n",
    "        ngram_range=(1, 1),max_features=10000)\n",
    "\n",
    "    word_vectorizer.fit(df['review_clean'])\n",
    "    \n",
    "    # train - test vectorized features - tranforming to suitable format for modeling\n",
    "    train_word_features = word_vectorizer.transform(X_train) \n",
    "    test_word_features = word_vectorizer.transform(X_test)\n",
    "    \n",
    "    return train_word_features , test_word_features, y_train, y_test \n",
    "\n",
    "def model(df,classifier):\n",
    "    '''this function gives modeling results and confusion matrix also'''\n",
    "    train_word_features,test_word_features,y_train,y_test = model_split(df)\n",
    "    classifier.fit(train_word_features, y_train)\n",
    "    \n",
    "    # calculating results \n",
    "    y_pred_train = classifier.predict(train_word_features)\n",
    "    y_pred = classifier.predict(test_word_features)\n",
    "    \n",
    "    #for smart printing (learned from our lead instructor Bryan Arnold)\n",
    "    print(\"Accuracy:\"); print(\"=\"*len(\"Accuracy:\"))\n",
    "    print(f\"TRAIN: {accuracy_score(y_train, y_pred_train)}\")\n",
    "    print(f\"TEST: {accuracy_score(y_test, y_pred)}\")\n",
    "\n",
    "    print(\"\\nBalanced Accuracy:\"); print(\"=\"*len(\"Balanced Accuracy:\"))\n",
    "    print(f\"TRAIN: {balanced_accuracy_score(y_train, y_pred_train)}\")\n",
    "    print(f\"TEST: {balanced_accuracy_score(y_test, y_pred)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23846,)\n",
      "(5962,)\n",
      "Accuracy:\n",
      "=========\n",
      "TRAIN: 0.9537029271156588\n",
      "TEST: 0.943139885944314\n",
      "\n",
      "Balanced Accuracy:\n",
      "==================\n",
      "TRAIN: 0.9618165729065065\n",
      "TEST: 0.889112221532667\n"
     ]
    }
   ],
   "source": [
    "# choosing classifier and running model\n",
    "classifier = LogisticRegression(class_weight = \"balanced\", C=0.5, solver='sag')\n",
    "model(df_sampled,classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With simple Logreg, I got my baseline model for 2-balanced class data. I do not work on too much this model, because it is my baseline. I will decide after other models. My main concern is this matrix are wrong predictions on 0. I will try to find the best results according to accuracy and true 0 class predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One More Check to See roc-auc Score with Cross-Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9277745442741944 +/- 0.04463079731926346\n"
     ]
    }
   ],
   "source": [
    "# vectorize data for calculation\n",
    "word_vectorizer = TfidfVectorizer(sublinear_tf=True,strip_accents='unicode',\n",
    "        analyzer='word',token_pattern=r'\\w{1,}',stop_words='english',\n",
    "        ngram_range=(1, 1),max_features=10000)\n",
    "\n",
    "word_vectorizer.fit(df_sampled['review_clean'])\n",
    "    \n",
    "word_features = word_vectorizer.transform(df_sampled['review_clean'])\n",
    "\n",
    "scores = cross_val_score(classifier, word_features, df_sampled['user_sentiment'], cv=3, scoring='roc_auc')\n",
    "print(scores.mean(), \"+/-\", scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have pretty good results for baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree classifier is a supervised classification technique which splits data according to certain parameters continuously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23846,)\n",
      "(5962,)\n",
      "Accuracy:\n",
      "=========\n",
      "TRAIN: 0.9606223265956555\n",
      "TEST: 0.961590070446159\n",
      "\n",
      "Balanced Accuracy:\n",
      "==================\n",
      "TRAIN: 0.7070450831962745\n",
      "TEST: 0.6933852639688416\n"
     ]
    }
   ],
   "source": [
    "des_tree = DecisionTreeClassifier(criterion='entropy', max_depth=4, min_samples_leaf=5, \n",
    "                                        random_state=42, class_weight='balanced')\n",
    "model(df_sampled,des_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I do not find better results than baseline when I try to tune this model, so I continue to try other models. With simple Logreg, my test accuracy was 87, but this one gives 0.69."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra-Trees Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra Trees works like a Random Forest. It builds multiple trees and splits nodes using random subsets of features. There are two main differences it does not samples without replacement, and nodes are split on random splits, not best splits. I also used different parameters and tuned them manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23846,)\n",
      "(5962,)\n",
      "Accuracy:\n",
      "=========\n",
      "TRAIN: 0.9332802147110627\n",
      "TEST: 0.9315665883931566\n",
      "\n",
      "Balanced Accuracy:\n",
      "==================\n",
      "TRAIN: 0.8300947880060237\n",
      "TEST: 0.8135408418754949\n"
     ]
    }
   ],
   "source": [
    "rf_extra = ExtraTreesClassifier(max_depth=5, criterion= 'entropy', min_samples_leaf=3, min_samples_split=18, \n",
    "                          random_state=42, n_estimators = 100, class_weight='balanced', n_jobs = -1)\n",
    "\n",
    "model(df_sampled,rf_extra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got better than decision tree but not better than baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest models can reduce overfitting risk by randomness as building n_estimators, bootstrapping sample and splitting nodes on the best split among a random subset of the features selected at every node and converting non-homogeneous node into best possibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23846,)\n",
      "(5962,)\n",
      "Accuracy:\n",
      "=========\n",
      "TRAIN: 0.9997064497190304\n",
      "TEST: 0.972660181147266\n",
      "\n",
      "Balanced Accuracy:\n",
      "==================\n",
      "TRAIN: 0.999843715114981\n",
      "TEST: 0.7658557854865287\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1,class_weight='balanced',\n",
    "                            criterion = 'entropy',max_features = 'sqrt',min_samples_split = 5)\n",
    "model(df_sampled,rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest give high accuracy but it is overfit. It could not split my target well. I can find best parameters for my model and tune it with grid search. But, it took too much time, so I prefered to changed parameters manually and run for each time. I got the best results with this parameters. There is grid search code below to show how to find best parameters and best score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=RandomForestClassifier(class_weight='balanced',\n",
       "                                              criterion='entropy',\n",
       "                                              max_features='sqrt',\n",
       "                                              min_samples_split=5, n_jobs=-1,\n",
       "                                              random_state=42),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'criterion': ['gini', 'entropy'],\n",
       "                         'max_features': ['sqrt', 'log2'],\n",
       "                         'min_samples_split': [2, 5, 10],\n",
       "                         'n_estimators': [20, 50, 100]},\n",
       "             scoring='roc_auc')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " grid_p = {\"n_estimators\": [20, 50, 100],\n",
    "           \"criterion\": [\"gini\", \"entropy\"],\n",
    "           \"max_features\": ['sqrt', 'log2'],\n",
    "           \"min_samples_split\": [2, 5, 10]}\n",
    "\n",
    " grid_search = GridSearchCV(rf, grid_p, n_jobs=-1, cv=3, scoring='roc_auc')\n",
    " grid_search.fit(word_features, df_sampled['user_sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9132323733319669"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost is an efficient and flexiable classifier which implements models with Gradient Boosting. Although, it is very usefull classifier, it can be overfit easily. So, parameter selection is very imporant for XGBoost. I can do grid search also for XGBoost to tune our parameters but it takes too much time. So, I played around our parameters manually and decided the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23846,)\n",
      "(5962,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swammy\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:54:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:573: \n",
      "Parameters: { \"early_stopping_rounds\", \"maximize\", \"nrounds\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "Accuracy:\n",
      "=========\n",
      "TRAIN: 0.9854063574603708\n",
      "TEST: 0.9721569942972157\n",
      "\n",
      "Balanced Accuracy:\n",
      "==================\n",
      "TRAIN: 0.8800827015851137\n",
      "TEST: 0.7627549273470435\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(objective = 'multi:softmax', booster = 'gbtree', nrounds = 'min.error.idx',\n",
    "                      num_class = 3, maximize = False, eval_metric = 'merror',early_stopping_rounds=10,\n",
    "                        eta = .1,max_depth = 12, colsample_bytree = .4, learning_rate = 0.1,\n",
    "                        max_delta_step=1)\n",
    "model(df_sampled, xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gives good results but not better than baseline. The most important parameters are learning rate, early stopping round, max_depth and max_delta step for my tuning. When I changed them, results significantly changed. So, I set this parameters according to results which I found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LGBM also constructs a gradient boosting model. It has some advantages that faster training speed and higher efficiency with lower memory usage. It is capable large data sets with significantly quicker than XGBoost. So, I tried this one also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23846,)\n",
      "(5962,)\n",
      "[LightGBM] [Warning] Unknown parameter: booster\n",
      "[LightGBM] [Warning] Unknown parameter: nrounds\n",
      "[LightGBM] [Warning] Unknown parameter: maximize\n",
      "[LightGBM] [Warning] learning_rate is set=0.1, eta=0.1 will be ignored. Current value: learning_rate=0.1\n",
      "Accuracy:\n",
      "=========\n",
      "TRAIN: 0.9736643462215885\n",
      "TEST: 0.9706474337470647\n",
      "\n",
      "Balanced Accuracy:\n",
      "==================\n",
      "TRAIN: 0.7916541070981526\n",
      "TEST: 0.7520351387789167\n"
     ]
    }
   ],
   "source": [
    "lgbm = LGBMClassifier(booster = 'gbtree', nrounds = 'min.error.idx', maximize = False,eta = .1,max_depth = 10, \n",
    "                      colsample_bytree = .4,learning_rate = 0.1,max_delta_step=1)\n",
    "model(df_sampled, lgbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gives higher than 80% but lower than baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to keep results and see in dataframe\n",
    "df_results = pd.DataFrame(columns=[\"Model\", 'train_balanced', 'test_balanced']) # to see all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = df_results.append({       # writing results to df\n",
    "     \"Model\": 'Logreg' ,\n",
    "               'train_balanced' : 0.96,\n",
    "                    'test_balanced' : 0.88}, ignore_index=True)\n",
    "df_results = df_results.append({       # writing results to df\n",
    "     \"Model\": 'Decision Tree' ,\n",
    "               'train_balanced' : 0.96,\n",
    "                    'test_balanced' : 0.88}, ignore_index=True)\n",
    "df_results = df_results.append({       # writing results to df\n",
    "     \"Model\": 'Extra-Tree' ,\n",
    "               'train_balanced' : 0.83,\n",
    "                    'test_balanced' : 0.81}, ignore_index=True)\n",
    "df_results = df_results.append({       # writing results to df\n",
    "     \"Model\": 'Random Forest' ,\n",
    "               'train_balanced' : 0.99,\n",
    "                    'test_balanced' : 0.76}, ignore_index=True)\n",
    "df_results = df_results.append({       # writing results to df\n",
    "     \"Model\": 'XGBM' ,\n",
    "               'train_balanced' : 0.88,\n",
    "                    'test_balanced' : 0.76}, ignore_index=True)\n",
    "df_results = df_results.append({       # writing results to df\n",
    "     \"Model\": 'LGBM' ,\n",
    "               'train_balanced' : 0.79,\n",
    "                    'test_balanced' : 0.75}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>train_balanced</th>\n",
       "      <th>test_balanced</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logreg</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Extra-Tree</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBM</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LGBM</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model  train_balanced  test_balanced\n",
       "0         Logreg            0.96           0.88\n",
       "1  Decision Tree            0.96           0.88\n",
       "2     Extra-Tree            0.83           0.81\n",
       "3  Random Forest            0.99           0.76\n",
       "4           XGBM            0.88           0.76\n",
       "5           LGBM            0.79           0.75"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to results, my baseline did better than others. I can work on to tune these models and can get better results. But, I would like to try and see how deep learning models will do. So, I drop it here and continue to model with NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "ax = sns.barplot(x='Model', y= 'test_balanced',data=df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Improvements for This Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I will not get good results with deep learning, the ideas below can be tried; \n",
    "- Gradient boost models can be tuned.\n",
    "- Grid search can be done.\n",
    "- I can come back to text column and do deeper analysis and cleaning and feature engineering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
